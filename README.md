# BTEST - Solutions Engineering Test Implementation

This repository contains the implementation of the solution for the technical challenge for the Solutions Engineer position. The system consists of a Docker-based microservices architecture with integration to language models (GPT), web automation, and PostgreSQL database.

## Architecture

The project consists of three main Docker services:

1. **automation-service**: NestJS service responsible for automated data extraction through Puppeteer and sending it for GPT processing.
2. **gpt-service**: NestJS service that provides endpoints for load summarization using GPT Compute Preview.
3. **postgres**: PostgreSQL database for storing data and reports.

## Prerequisites

- Docker and Docker Compose
- Node.js (for local development)
- OpenAI API Key (for the GPT service)

## Execution Instructions

### Environment Variable Configuration

1. Create a `.env` file in the project root with the following content:
   ```
   OPENAI_API_KEY=your_api_key_here
   ```

### Build and Initialization

```bash
# Build images
docker compose build

# Start services
docker compose up -d

# Check status
docker compose ps
```

### Health Verification

```bash
# Check logs
docker compose logs

# Check automation-service health
curl http://localhost:3000/metrics

# Check gpt-service health
curl http://localhost:3001/health
```

### Stopping Services

```bash
# Stop services
docker compose down

# To remove volumes (clear database)
docker compose down -v
```

## Database Structure

The PostgreSQL database contains the following tables:

- **drivers**: Stores information about drivers
- **loads**: Stores data about loads (origin, destination, price, ETA)
- **summaries**: Stores summaries generated by GPT from load data

There is also a materialized view **load_summary_view** that combines data from `loads` and `summaries` for quick queries.

## API Endpoints

### GPT Service

**POST /summarize-loads**

Receives an array of load objects and returns a GPT-generated summary.

Request example:

```json
[
  {
    "origin": "New York NY",
    "destination": "Los Angeles CA",
    "price": 5000,
    "eta": "2023-12-31"
  }
]
```

Response example:

```json
{
  "summary": "This load goes from New York NY to Los Angeles CA with a value of $5,000 and an estimated arrival date of 12/31/2023.",
  "insights": [
    "Transcontinental route with above-average pricing",
    "Consider consolidation with other loads on the same route"
  ]
}
```

### Automation Service

**GET /metrics**

Endpoint that exposes Prometheus metrics for monitoring.

**GET /load-insights**

Returns the top 5 loads with their insights, querying the database's materialized view.

## Tests

```bash
# Run tests for automation-service
cd automation-service
npm test

# Run tests for gpt-service
cd gpt-service
npm test
```

## Development and Challenges

This project implemented all the requirements of the technical challenge, including:

1. Microservices architecture with Docker Compose
2. Data extraction automation via Puppeteer
3. Integration with the GPT Compute Preview API
4. Data modeling and orchestration with PostgreSQL
5. Complete documentation

### Post-mortem (300 words)

During the development of this project, we faced several technical challenges and made important architectural decisions to ensure a robust and scalable solution.

**Challenges:**

1. **Service Synchronization**: The correct configuration of dependencies between services using `depends_on` and `healthcheck` was crucial to ensure that the `automation-service` only started after the database and `gpt-service` were fully functional.

2. **Database Initialization**: Ensuring that the SQL schema was correctly applied during PostgreSQL container initialization required precise configuration of volumes and initialization scripts.

3. **Resilient Automation**: Extracting data from load portals using Puppeteer required implementing sophisticated retry and backoff mechanisms to handle transient failures.

4. **OpenAI Integration**: Implementing fallbacks and quota management was necessary to ensure the robustness of the summarization service.

**Architectural Decisions:**

1. **Materialized View**: We chose to create a materialized view (`load_summary_view`) to optimize frequent queries that combine load and summary data, significantly improving the performance of the `/load-insights` endpoint.

2. **Shared Logs**: We used shared volumes for logs, facilitating the diagnosis of issues between services.

3. **Separation of Concerns**: The clear separation between the automation service and the GPT processing service allows for independent scaling and simpler maintenance.

**Next Steps:**

1. Implement CI/CD with GitHub Actions for test and deployment automation
2. Expand observability with complete Prometheus/Grafana integration
3. Add multitenancy support to serve different clients
4. Implement a caching service to reduce calls to the OpenAI API

This implementation demonstrates the ability to create resilient distributed systems that combine automation, natural language processing, and data orchestration in a modern microservices architecture.

â‚¢2025 - Rene J. Martins.
